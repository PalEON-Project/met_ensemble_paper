# -------------------------------------------
# Script to identify and create a prioritization list of ensemble members to use to test sensitivity of ecosystem models to weather vs. climate
# The met analyzed here generated by splicing together many data products and propogating uncertainty & variable covariance
# The Github Repository for the workflow for this can be found here: https://github.com/PalEON-Project/modeling_met_ensemble
#
# Notes: Until we get empirical met filtering working, we'll just use members whose PDSI doesn't go INSANE and is "representative" of the GCM
# For each GCM, Identify the ensemble members that most accurately represent the mean
# -------------------------------------------

# -------------------------------------------
# 1. Identify inputs, outputs etc.
# -------------------------------------------
# The file path to where our ensembel is stored
path.dat <- "~/met_ensemble/data/met_ensembles/HARVARD/1hr/ensembles"
# path.out <- "../data/"
# path.google <- "~/Google Drive/PalEON_Met_Ensembles/"
# -------------------------------------------


# -------------------------------------------
# Go by GCM
# -------------------------------------------
GCM.list <- dir(path.dat)

# ens.mems <- vector()
# n.files <- 0
# vars.names <- vector()
ens.order <- list()
for(GCM in GCM.list){
  ens.mems <- dir(file.path(path.dat, GCM))
  
  # Setting up some general indices
  files.now <- dir(file.path(path.dat, GCM, ens.mems[1]))
  n.files <- length(files.now)
  
  ncT <- ncdf4::nc_open(file.path(path.dat, GCM, ens.mems[1], files.now[1]))
  var.names <- names(ncT$var)
  ncdf4::nc_close(ncT)
  
  # We want to find the best ensemble member that takes into account the hourly fluxes, so lets go year by year
  files.now <- list()
  for(ens in ens.mems){
    f.ens <- dir(file.path(path.dat, GCM, ens))
    
    if(length(f.ens) < length(850:2015)) next # Skip ensemble members missing years

    files.now[[ens]] <- f.ens
  }
  
  ens.mems <- names(files.now) # Replace our ensemble with only what we have
  
  # Setting out where to store the info from our ensemble members
  dat.summary <- array(dim=c(n.files, length(ens.mems))) 
  dimnames(dat.summary)[[1]] <- seq(850, length.out=n.files, by=1)
  dimnames(dat.summary)[[2]] <- ens.mems
  names(dimnames(dat.summary)) <- c("Year", "ensemble.member")
  
  
  # Finding the best model year by year
  pb <- txtProgressBar(min=0, max=length(files.now[[1]]), style=3)
  for(i in 1:length(files.now[[1]])){
    ncT <- ncdf4::nc_open(file.path(path.dat, GCM, ens.mems[1], files.now[[1]][i]))
    dat.temp <- array(dim=c(ncT$dim$time$len, length(ncT$var), length(ens.mems)))
    ncdf4::nc_close(ncT)
    
    for(j in 1:length(ens.mems)){
      ncT <- ncdf4::nc_open(file.path(path.dat, GCM, ens.mems[j], files.now[[j]][i]))
      for(v in 1:length(ncT$var)){
        dat.temp[,v,j] <- ncdf4::ncvar_get(ncT, names(ncT$var)[v])
      }
      ncdf4::nc_close(ncT)
    }
    
    # Calculate the residuals
    dat.means <- apply(dat.temp, c(1,2), FUN=mean)
    # dat.resid <- array(dim=dim(dat.temp))
    for(v in 1:dim(dat.means)[2]){
      dat.temp[,v,] <- dat.temp[,v,] - dat.means[,v]
    }
    
    # Use sum of squares to find the which is best
    df.ss <- apply(dat.temp, c(2,3), function(x) sum(x^2))
    
    # Relativize the sum of squares fo variables get the same weight
    df.ss <- abs(df.ss-rowMeans(df.ss))/rowMeans(df.ss)
    ens.wt <- apply(df.ss, 2, sum)
    
    dat.summary[i,] <- ens.wt
    
    setTxtProgressBar(pb, i)
  }
  
  ens.wts <- apply(dat.summary, 2, sum)
  ens.order[[GCM]] <- ens.wts[order(ens.wts)]
  
}

